import requests
from bs4 import BeautifulSoup
import csv

# Define the URL template for Amazon product listing pages
base_url = "https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_{}"

# Number of pages to scrape
num_pages = 20

# Initialize an empty list to store the scraped data
data = []

# Loop through the specified number of pages
for page_num in range(1, num_pages + 1):
    url = base_url.format(page_num)
    response = requests.get(url)
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Find the product listings on the page
        product_listings = soup.find_all("div", class_="s-result-item")
        
        for listing in product_listings:
            product_url = listing.find("a", class_="a-link-normal")["href"]
            product_name = listing.find("span", class_="a-text-normal").get_text(strip=True)
            product_price = listing.find("span", class_="a-price-whole").get_text(strip=True)
            
            try:
                rating = listing.find("span", class_="a-icon-alt").get_text(strip=True)
            except AttributeError:
                rating = "N/A"
            
            try:
                num_reviews = listing.find("span", class_="a-size-base").get_text(strip=True)
            except AttributeError:
                num_reviews = "N/A"
            
            data.append([product_url, product_name, product_price, rating, num_reviews])
    else:
        print(f"Failed to fetch data from page {page_num}")

# Save the data to a CSV file
with open("amazon_product_data.csv", "w", newline="", encoding="utf-8") as csv_file:
    writer = csv.writer(csv_file)
    writer.writerow(["Product URL", "Product Name", "Product Price", "Rating", "Number of Reviews"])
    writer.writerows(data)

print("Data has been successfully scraped and saved to amazon_product_data.csv.")
